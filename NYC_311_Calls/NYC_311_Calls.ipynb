{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ideas for importing data:\n",
    "#1. get data in batches: export to a list, export list to a csv, import csv for each new run - do data preprocessing\n",
    "#2. get data in batches, process each batch, export to a csv, import csv with specified datatypes\n",
    "#3. how big is a \"big\" dataset? whats the general limit for what i can export to work with on my computer?\n",
    "#4. multithreading vs. multiprocessing?\n",
    "#5. export data to a database? SQLalchemy\n",
    "#6. work directly in colab, then download and export to github?\n",
    "#\n",
    "#call with ori:\n",
    "#options:\n",
    "#1. download locally, wait for however long it takes, large = when it stops fitting in ram\n",
    "#mem = temp working space, fast to access but can't store much, might be erased; disk = long term storage\n",
    "#goal = get something done quickly\n",
    "#embarrassingly parallel problems: processes don't need to talk to each other \n",
    "\n",
    "#2. BQ: data stays in google servers, might have to pay eventually, if you need to do a lot of processing / really large datasets\n",
    "#take the processed results \n",
    "#use api to query from BQ\n",
    "\n",
    "#3. spark cluster on AWS\n",
    "\n",
    "#keep raw data - quickly recover\n",
    "\n",
    "#python stores each num as an obj; each obj has overhead involved (methods, value, ref count)\n",
    "#numpy stores all of these values as one obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do:\n",
    "- multiprocessing or multithreading to get more data\n",
    "- save data to csv, use pandas to read in and indicate appropriate data types\n",
    "- data processing: add time, drop nulls\n",
    "- reduce memory\n",
    "- create cleaner categories; create function to indicate any new categories that need to be coded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset, reduce memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from sodapy import Socrata\n",
    "import os\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "from fuzzywuzzy import process\n",
    "from fuzzywuzzy import fuzz\n",
    "import pygsheets\n",
    "from datetime import datetime, date, time \n",
    "import json\n",
    "\n",
    "from bokeh.plotting import figure, output_file, output_notebook, show, save, reset_output, gmap\n",
    "from bokeh.models import ColumnDataSource, GMapOptions, HoverTool, BasicTicker, ColorBar, LinearColorMapper, PrintfTickFormatter, Panel, Tabs\n",
    "from bokeh.palettes import Spectral6, all_palettes, brewer\n",
    "from bokeh.transform import factor_cmap, transform, linear_cmap\n",
    "from bokeh.layouts import column, row, layout, WidgetBox\n",
    "from bokeh.io import output_file, show\n",
    "from bokeh.sampledata.unemployment1948 import data\n",
    "from bokeh.models import CheckboxButtonGroup, CheckboxGroup, TextInput\n",
    "\n",
    "os.getcwd()\n",
    "pd.options.display.max_columns=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'count': '718016'}]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define parameters for endpoint, dataset, and app token\n",
    "data_url = 'data.cityofnewyork.us'\n",
    "dataset = 'erm2-nwe9'\n",
    "app_token = 'dM7DDeidbAmgtydtJVV1epbiU'\n",
    "\n",
    "#sets up the connection, need application token to override throttling limits\n",
    "#username and password only required for creating or modifying data\n",
    "client = Socrata(data_url, app_token)\n",
    "client.timeout = 6000\n",
    "\n",
    "#count number of records in desired dataset\n",
    "record_count = client.get(dataset, select='count(*)', where=\"created_date >='2020-01-01'\")\n",
    "record_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(chunk_size=1000, total_rows=100000):\n",
    "    start = 0\n",
    "    results=[]\n",
    "\n",
    "    #paginate through dataset in sets of 10000 to get all records since 2019\n",
    "    while True:\n",
    "        print(f'{start} rows retrieved')\n",
    "        results.extend(client.get(dataset,where=\"created_date >= '2019-11-01'\", \n",
    "                                  limit=chunk_size, offset=start))\n",
    "        start += chunk_size\n",
    "        if start > total_rows:\n",
    "            break\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 rows retrieved\n",
      "1000 rows retrieved\n",
      "2000 rows retrieved\n",
      "3000 rows retrieved\n",
      "4000 rows retrieved\n",
      "5000 rows retrieved\n",
      "6000 rows retrieved\n",
      "7000 rows retrieved\n",
      "8000 rows retrieved\n",
      "9000 rows retrieved\n",
      "10000 rows retrieved\n",
      "11000 rows retrieved\n",
      "12000 rows retrieved\n",
      "13000 rows retrieved\n",
      "14000 rows retrieved\n",
      "15000 rows retrieved\n",
      "16000 rows retrieved\n",
      "17000 rows retrieved\n",
      "18000 rows retrieved\n",
      "19000 rows retrieved\n",
      "20000 rows retrieved\n",
      "21000 rows retrieved\n",
      "22000 rows retrieved\n",
      "23000 rows retrieved\n",
      "24000 rows retrieved\n",
      "25000 rows retrieved\n",
      "26000 rows retrieved\n",
      "27000 rows retrieved\n",
      "28000 rows retrieved\n",
      "29000 rows retrieved\n",
      "30000 rows retrieved\n",
      "31000 rows retrieved\n",
      "32000 rows retrieved\n",
      "33000 rows retrieved\n",
      "34000 rows retrieved\n",
      "35000 rows retrieved\n",
      "36000 rows retrieved\n"
     ]
    }
   ],
   "source": [
    "orig_results = get_data()\n",
    "orig_df = pd.DataFrame(orig_results)\n",
    "orig_df.to_csv('/Users/linyu/Documents/Python/data/311_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_memory(df):\n",
    "    \"\"\"improved version of memory reduction function. uses pd.to_numeric to downcast types;\n",
    "    also considers whether there are few enough unique values to warrant use of category vs. object.\"\"\"\n",
    "    orig_size = df.memory_usage().sum()/1024**2\n",
    "    dtypes = df.dtypes.astype(str).unique()\n",
    "    converted_float = pd.DataFrame()\n",
    "    converted_int = pd.DataFrame()\n",
    "    converted_obj = pd.DataFrame()\n",
    "    converted_misc = pd.DataFrame()\n",
    "\n",
    "    #convert floats\n",
    "    selected_float = df.select_dtypes(include='float')\n",
    "    converted_float = selected_float.apply(pd.to_numeric, downcast='float')\n",
    "    float_size = selected_float.memory_usage().sum()/1024**2\n",
    "    converted_float_size = converted_float.memory_usage().sum()/1024**2\n",
    "    print(f'floats: {float_size:.2f} reduced to {converted_float_size:.2f} MB')\n",
    "\n",
    "    #convert ints\n",
    "    selected_int = df.select_dtypes(include='integer')\n",
    "    converted_int = selected_int.apply(pd.to_numeric, downcast='integer')\n",
    "    int_size = selected_int.memory_usage().sum()/1024**2\n",
    "    converted_int_size = converted_int.memory_usage().sum()/1024**2\n",
    "    print(f'ints: {int_size:.2f} reduced to {converted_int_size:.2f} MB')\n",
    "    \n",
    "    #convert objects / categories\n",
    "    selected_object = df.select_dtypes(include=['object', 'category'])\n",
    "    obj_size = selected_object.memory_usage().sum()/1024**2\n",
    "    for col in selected_object.columns:\n",
    "        count = len(selected_object[col])\n",
    "        unique = len(selected_object[col].astype(str).unique())\n",
    "        if unique < count/2:\n",
    "            converted_obj[col] = selected_object[col].astype(str).astype('category')\n",
    "        else:\n",
    "            converted_obj[col] = selected_object[col].astype(str)\n",
    "    converted_obj_size = converted_obj.memory_usage().sum()/1024**2\n",
    "    print(f'object: {obj_size:.2f} reduced to {converted_obj_size:.2f} MB')\n",
    "\n",
    "    #join floats, ints, and objects / categories\n",
    "    float_int = converted_float.join(converted_int)\n",
    "    float_int_obj = float_int.join(converted_obj)\n",
    "    \n",
    "    #for any columns of any other type, keep them the same and join to the converted dataframe\n",
    "    no_change_cols = [x for x in df.columns if x not in float_int_obj.columns]\n",
    "    reduced_df = float_int_obj.join(df[no_change_cols])\n",
    "    \n",
    "    #re-order columns to appear in original order\n",
    "    reduced_df = reduced_df[df.columns]\n",
    "    reduced_size = reduced_df.memory_usage().sum()/1024**2\n",
    "    print(f'final df: {orig_size:.2f} reduced to {reduced_size:.2f} MB, {(orig_size-reduced_size)/orig_size*100:.1f}% reduction')\n",
    "    return reduced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "floats: 0.50 reduced to 0.25 MB\n",
      "ints: 0.08 reduced to 0.04 MB\n",
      "object: 2.85 reduced to 1.46 MB\n",
      "final df: 3.44 reduced to 1.75 MB, 49.1% reduction\n"
     ]
    }
   ],
   "source": [
    "#olddf - most dtypes are objects, some are ints and some are floats\n",
    "#BUT olddf datatypes get changed inplace after running reduce_mem_usage! becomes mostly categorical\n",
    "olddf = pd.read_csv('/Users/linyu/Documents/Python/data/311_data.csv')\n",
    "\n",
    "#newdf - most dtypes are categorical, some ints and floats\n",
    "newdf = reduce_memory(olddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #need to fix this - categorical blanks are not showing up as blanks\n",
    "# plt.rc('figure',figsize=(15,4))\n",
    "# #display(sns.heatmap(olddf.isnull()))\n",
    "# display(sns.heatmap(newdf.isnull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "- need to create a function that will show the new, unmapped categories for each new imported dataset\n",
    "- need to indicate column types upon reading in with pandas, use the reduce memory function after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df):\n",
    "    '''drop columns that are mostly blank, create time-related columns from date columns'''\n",
    "    #drop mostly blank columns\n",
    "    cleandf = df.drop(['intersection_street_1',\n",
    "               'intersection_street_2',\n",
    "               'taxi_company_borough',\n",
    "               'taxi_pick_up_location',\n",
    "               'bridge_highway_name',\n",
    "               'bridge_highway_direction',\n",
    "               'road_ramp',\n",
    "               'bridge_highway_segment',\n",
    "               'landmark',\n",
    "               'vehicle_type'], axis=1, inplace=False)\n",
    "\n",
    "    #convert date related columns to datetime\n",
    "    for col in cleandf.columns:\n",
    "        if 'date' in col:\n",
    "            cleandf[col] = pd.to_datetime(cleandf[col])\n",
    "            \n",
    "    #created time-related columns from date columns\n",
    "    cleandf['created_mdy'] = [x.strftime('%Y-%m-%d') for x in cleandf['created_date']]\n",
    "    cleandf['created_year'] = [x.year for x in cleandf['created_date']]\n",
    "    cleandf['created_month'] = [x.strftime('%b') for x in cleandf['created_date']]\n",
    "    cleandf['created_day'] = [x.day for x in cleandf['created_date']]\n",
    "    cleandf['created_weekday'] = [x.strftime('%a') for x in cleandf['created_date']]\n",
    "    cleandf['created_week'] = [x.week for x in cleandf['created_date']]\n",
    "    cleandf['created_hour'] = [x.hour for x in cleandf['created_date']]\n",
    "    cleandf['closed_hour'] = [x.hour for x in cleandf['closed_date']]\n",
    "    cleandf['time_to_close'] = cleandf['closed_date'] - cleandf['created_date']\n",
    "    cleandf['count'] = 1\n",
    "    return cleandf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_col_values(workbook, df, columns):\n",
    "    \"\"\"for a list of columns, creates a new sheet for each column and exports unique values and their counts to that sheet\"\"\"\n",
    "    for col in columns:\n",
    "        value_counts = df[col].value_counts()\n",
    "        counts_df = pd.DataFrame(value_counts).reset_index()\n",
    "        #was getting an error with using fillna for categorical column, need to cast to string\n",
    "        counts_df['index'] = counts_df['index'].astype(str)\n",
    "        try:\n",
    "            worksheet = workbook.worksheet_by_title(col)\n",
    "        except Exception:\n",
    "            #ensure the error is in regards to missing the worksheet\n",
    "            print(sys.exc_info())\n",
    "            workbook.add_worksheet(col)\n",
    "            worksheet = workbook.worksheet_by_title(col)\n",
    "        worksheet.set_dataframe(counts_df, start='A1')\n",
    "    print(f'{len(columns)} sets of column values exported.')\n",
    "        \n",
    "def get_valid_names(workbook, columns, start='D1'):\n",
    "    \"\"\"extracts the valid names manually entered by the user in column D of the workbook\"\"\"\n",
    "    valid_names = {}\n",
    "    for col in columns:\n",
    "        worksheet = workbook.worksheet_by_title(col)\n",
    "        valid_matrix = worksheet.get_values(start='D1', end='D100')\n",
    "        valid_names[col] = [v[0] for v in valid_matrix]\n",
    "    return valid_names\n",
    "\n",
    "def fuzzy_match(value):\n",
    "    \"\"\"returns the best match for each column; fuzzy match score of < 90 will return 'Other'\"\"\"\n",
    "    match = process.extract(query=value, choices=valid_names[col], limit=1)\n",
    "    if match[0][1] < 90:\n",
    "        return 'Other'\n",
    "    else:\n",
    "        return match[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 sets of column values exported.\n"
     ]
    }
   ],
   "source": [
    "#use pygsheets to connect to workbook where we will export unique column values\n",
    "client = pygsheets.authorize(service_account_file='/Users/linyu/Documents/Python/data/client_secret.json')\n",
    "workbook = client.open('311_data_cleaning')\n",
    "columns = ['agency_name','complaint_type','descriptor','location_type','city']\n",
    "\n",
    "#export unique column values and their counts\n",
    "export_col_values(workbook, newdf, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMMENT OUT THIS CELL IF DOING CATEGORY REVIEW\n",
    "#get dictionary of lists with valid names for each column\n",
    "valid_names = get_valid_names(workbook, columns, start='D1')\n",
    "\n",
    "#fuzzy match each of the columns to the available values\n",
    "for col in columns:\n",
    "    newdf['cleaned_'+col] = newdf[col].apply(fuzzy_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "floats: 0.08 reduced to 0.04 MB\n",
      "ints: 0.55 reduced to 0.19 MB\n",
      "object: 1.43 reduced to 0.85 MB\n",
      "final df: 2.65 reduced to 1.66 MB, 37.1% reduction\n"
     ]
    }
   ],
   "source": [
    "#preprocess dataset to remove mostly null columns and create date columns\n",
    "processed_df = preprocess_df(newdf)\n",
    "#nyc_311_calls = reduce_memory(processed_df)\n",
    "clean_processed = reduce_memory(processed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bokeh Visualizations\n",
    "- function to create bar graphs\n",
    "- function to create tables\n",
    "- function to create heatmaps\n",
    "- function to create geo map\n",
    "- function to create call outs\n",
    "- add visualizations and sliders to dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BokehBarGraph:\n",
    "    def __init__(self, category, value, num_categories=9):\n",
    "        self.category = category\n",
    "        self.value = value\n",
    "        self.num_categories = num_categories\n",
    "        self.dataset = self.make_dataset()\n",
    "        \n",
    "    def make_dataset(self):\n",
    "        category = self.category\n",
    "        value = self.value\n",
    "        num_categories = self.num_categories\n",
    "        grouped_df = pd.DataFrame(clean_processed.groupby(category)[value].sum()).reset_index()[0:num_categories]\n",
    "        grouped_df[category] = grouped_df[category].astype(str)\n",
    "        grouped_df = grouped_df.sort_values(value, ascending=True)\n",
    "        try:\n",
    "            grouped_df['color'] = brewer['YlGnBu'][num_categories]\n",
    "        except KeyError:\n",
    "            print('Too many categories selected; select up to 9 categories to display.')\n",
    "        return grouped_df\n",
    "    \n",
    "    def make_plot(self):\n",
    "        category = self.category\n",
    "        value = self.value\n",
    "        dataset = self.dataset\n",
    "        #output_file('bargraph.html')\n",
    "        source = ColumnDataSource(dataset)\n",
    "\n",
    "        bargraph = figure(y_range=dataset[category], plot_height = 300, plot_width=600, background_fill_color=\"#000000\",\n",
    "                   x_axis_label=value, y_axis_label=category)\n",
    "        bargraph.grid.visible=False\n",
    "        \n",
    "#         color_map = factor_cmap(field_name=category, palette = Spectral6, \n",
    "#                                 factors=value_count[category].unique())\n",
    "        bargraph.hbar(y=category,right=value,source=source, height=.8, color='color', hover_color='white')\n",
    "        category_value = f'@{category}'\n",
    "        value_value = f'@{value}'\n",
    "        hover = HoverTool(tooltips=[(category,category_value),(value, value_value)])\n",
    "        bargraph.add_tools(hover)\n",
    "        return bargraph\n",
    "        #show(p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BokehHeatmap:\n",
    "    def __init__(self, x, y, value, exclude=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.value = value\n",
    "        self.exclude = exclude\n",
    "        self.pivot = self.make_dataset()[0]\n",
    "        self.df_pivot = self.make_dataset()[1]\n",
    "        \n",
    "    def make_dataset(self):\n",
    "        value = self.value\n",
    "        x=self.x\n",
    "        y=self.y\n",
    "        exclude = self.exclude\n",
    "        pivot = clean_processed.pivot_table(values=value,index=x,columns=y,aggfunc='sum')\n",
    "        if exclude:\n",
    "            try:\n",
    "                pivot = pivot.drop(exclude, axis=0)\n",
    "                print(f'{exclude} dropped from x-axis.')\n",
    "            except KeyError:\n",
    "                pivot = pivot.drop(exclude, axis=1)\n",
    "                print(f'{exclude} dropped from y-axis.')\n",
    "            except:\n",
    "                print('Exclusion does not exist in index or columns.')\n",
    "        pivot.columns = pivot.columns.astype(str)\n",
    "        pivot.index = pivot.index.astype(str)\n",
    "        list(pivot.index)\n",
    "        df_pivot = pd.DataFrame(pivot.stack(), columns=[value]).reset_index()\n",
    "        return pivot, df_pivot\n",
    "\n",
    "    def make_plot(self, title=None, x_ticks=None, y_ticks=None):\n",
    "        #reset_output()\n",
    "        #output_notebook()\n",
    "        df_pivot = self.df_pivot\n",
    "        pivot = self.pivot\n",
    "        x = self.x\n",
    "        y = self.y\n",
    "        value = self.value \n",
    "        source = ColumnDataSource(df_pivot)\n",
    "        \n",
    "        if title:\n",
    "            graph_title = title\n",
    "        else:\n",
    "            graph_title= f'{value} by {x} and {y}'\n",
    "        \n",
    "        if x_ticks:\n",
    "            x_range = x_ticks\n",
    "        else:\n",
    "            x_range = list(pivot.index)\n",
    "            \n",
    "        if y_ticks:\n",
    "            y_range = y_ticks\n",
    "        else:\n",
    "            y_range = list((pivot.columns))\n",
    "\n",
    "        colors = [\"#75968f\", \"#a5bab7\", \"#c9d9d3\", \"#e2e2e2\", \"#dfccce\", \"#ddb7b1\", \"#cc7878\", \"#933b41\", \"#550b1d\"]\n",
    "        mapper = LinearColorMapper(palette=colors, low=df_pivot[value].min(), high=df_pivot[value].max())\n",
    "\n",
    "        p = figure(plot_width=900, plot_height=500, title=graph_title,\n",
    "                   x_range=x_range, y_range=y_range,\n",
    "                   toolbar_location=None, x_axis_location=\"below\")\n",
    "\n",
    "        p.rect(x=x, y=y, width=1, height=1, source=source, line_color='white', fill_color=transform(value, mapper))\n",
    "        color_bar = ColorBar(color_mapper=mapper, location=(0, 0),\n",
    "                             ticker=BasicTicker(desired_num_ticks=len(colors)))\n",
    "        p.add_layout(color_bar, 'right')\n",
    "        \n",
    "        value_value = f'@{value}'\n",
    "        x_value = f'@{x}'\n",
    "        y_value = f'@{y}'\n",
    "\n",
    "        hover = HoverTool(tooltips=[(x, x_value),(y, y_value), (value, value_value)])\n",
    "        p.add_tools(hover)\n",
    "        p.grid.visible=False\n",
    "\n",
    "        p.axis.axis_line_color = None\n",
    "        p.axis.major_tick_line_color = None\n",
    "        p.axis.major_label_text_font_size = \"11px\"\n",
    "        p.axis.major_label_standoff = 0\n",
    "        p.xaxis.major_label_orientation = 1.0\n",
    "        return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geo Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BokehGeoMap():\n",
    "    def __init__(self, color='salmon', display_number=5000):\n",
    "        self.color = color\n",
    "        self.display_number = display_number\n",
    "        self.dataset = self.make_dataset()\n",
    "        \n",
    "    def make_dataset(self):\n",
    "        clean_processed['round_lat'] = round(clean_processed['latitude'],3)\n",
    "        clean_processed['round_lon'] = round(clean_processed['longitude'],3)\n",
    "        latlon_df = pd.DataFrame(clean_processed.groupby(['round_lat', 'round_lon'])['count'].sum()).reset_index()\n",
    "        latlon_df.sort_values('count', ascending=False)\n",
    "        latlon_df['scaled'] = (latlon_df['count']/latlon_df['count'].max()*50)\n",
    "        latlon_sorted = latlon_df.sort_values('scaled',ascending=False)\n",
    "        return latlon_sorted\n",
    "    \n",
    "    def make_plot(self):\n",
    "        #output_file('maps.html')\n",
    "        color = self.color\n",
    "        display_number = self.display_number\n",
    "        dataset = self.dataset\n",
    "        sample_df = dataset[:display_number]\n",
    "        \n",
    "        latitudes = sample_df['round_lat']\n",
    "        longitudes = sample_df['round_lon']\n",
    "        sizes = sample_df['scaled']\n",
    "\n",
    "        with open('/Users/linyu/Documents/Python/data/client_secret.json') as f:\n",
    "            data = json.load(f)\n",
    "        api_key = data[\"google_api_key\"]\n",
    "\n",
    "        map_options = GMapOptions(lat=40.76, lng=-73.95, map_type=\"roadmap\", zoom=11)\n",
    "        call_map = gmap(api_key, map_options, title=\"NYC 311 Calls\")\n",
    "        source = ColumnDataSource(data=dict(lat=latitudes,lon=longitudes, sizes = sizes))\n",
    "        call_map.circle(x=\"lon\", y=\"lat\", size='sizes', fill_color=color, fill_alpha=0.7, line_color=color, source=source)\n",
    "        return call_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geomap with slider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make_dataset() format specific data to be displayed\n",
    "#make_plot() draw plot with specified data\n",
    "#update() update plot based on user selections\n",
    "\n",
    "def geomap_tab(nyc_311_calls):\n",
    "    #function to make dataset for geomap based on number of points to display\n",
    "    #make_dataset function has the params that can be used to change the underlying data\n",
    "    #params: neighborhood, number of points\n",
    "    def make_dataset(boroughs, display_num=5000):\n",
    "        borough_filter = nyc_311_calls[nyc_311_calls['borough'].isin(boroughs)]\n",
    "        borough_filter['lat_round'] = round(borough_filter['latitude'],3)\n",
    "        borough_filter['lon_round'] = round(borough_filter['longitude'],3)\n",
    "        latlon_df = pd.DataFrame(borough_filter.groupby(['lat_round', 'lon_round'])['count'].sum()).reset_index()\n",
    "        latlon_df['sizes'] = latlon_df['count']/latlon_df['count'].max()*50\n",
    "        latlon_sorted = latlon_df.sort_values('sizes', ascending=False)\n",
    "        latlon_display = latlon_sorted[:display_num]\n",
    "        return ColumnDataSource(latlon_display)\n",
    "    \n",
    "    def make_plot(src):\n",
    "        #function to generate the plot given a ColumnDataSource with specific parameters\n",
    "        output_file('geomap_with_slider.html')\n",
    "        with open('/Users/linyu/Documents/Python/data/client_secret.json') as f:\n",
    "            data = json.load(f)\n",
    "        api_key = data['google_api_key']\n",
    "        map_options = GMapOptions(lat=40.76, lng=-73.95, map_type='roadmap', zoom=11)\n",
    "        call_map = gmap(api_key, map_options, title='NYC 311 Calls')\n",
    "        call_map.circle(x='lon_round', y='lat_round', size='sizes', source=src, fill_alpha = 0.7, \n",
    "                        fill_color='salmon', line_color='red')\n",
    "        return call_map\n",
    "    \n",
    "    def update(attr, old, new):\n",
    "        #function to update the underlying dataset given any change in interface widgets (checkbox, slider)\n",
    "        boroughs_to_plot = [borough_selection.labels[i] for i in borough_selection.active]\n",
    "        new_src = make_dataset(boroughs_to_plot, display_num=5000)\n",
    "        src.data.update(new_src.data)\n",
    "        \n",
    "    available_boroughs = list(set(nyc_311_calls['borough']))\n",
    "    available_boroughs.sort()\n",
    "    \n",
    "    borough_selection = CheckboxGroup(labels=available_boroughs, active=[0,1,2,3,4])\n",
    "    borough_selection.on_change('active', update)\n",
    "    \n",
    "    initial_boroughs = [borough_selection.labels[i] for i in borough_selection.active]\n",
    "    \n",
    "    src = make_dataset(initial_boroughs, display_num=5000)\n",
    "    p = make_plot(src)\n",
    "    controls = WidgetBox(borough_selection)\n",
    "    layout = row(controls, p)\n",
    "    tab = Panel(child=layout, title='Geomap')\n",
    "    tabs = Tabs(tabs=[tab])\n",
    "    return tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linyu/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/linyu/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "WARNING:bokeh.embed.util:\n",
      "You are generating standalone HTML/JS output, but trying to use real Python\n",
      "callbacks (i.e. with on_change or on_event). This combination cannot work.\n",
      "\n",
      "Only JavaScript callbacks may be used with standalone output. For more\n",
      "information on JavaScript callbacks with Bokeh, see:\n",
      "\n",
      "    https://docs.bokeh.org/en/latest/docs/user_guide/interaction/callbacks.html\n",
      "\n",
      "Alternatively, to use real Python callbacks, a Bokeh server application may\n",
      "be used. For more information on building and running Bokeh applications, see:\n",
      "\n",
      "    https://docs.bokeh.org/en/latest/docs/user_guide/server.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nyc_311_calls = clean_processed.copy()\n",
    "test = geomap_tab(nyc_311_calls = nyc_311_calls)\n",
    "show(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n",
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n",
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n",
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n",
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n",
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n",
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n",
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n",
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n",
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n",
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n",
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n",
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n",
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n",
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n",
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n",
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n",
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n",
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n",
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n",
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n",
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n",
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n"
     ]
    }
   ],
   "source": [
    "##line graph\n",
    "clean_processed['cleaned_descriptor'] = clean_processed['cleaned_descriptor'].astype(str)\n",
    "clean_processed['created_mdy'] = clean_processed['created_mdy'].astype(str)\n",
    "pivot = clean_processed.pivot_table(values='count', index='created_mdy', columns='cleaned_descriptor', aggfunc=sum).reset_index()\n",
    "source = ColumnDataSource(pivot)\n",
    "\n",
    "reset_output()\n",
    "linegraph = figure(x_range=pivot['created_mdy'])\n",
    "for col in pivot.columns:\n",
    "    linegraph.line(x='created_mdy', y = col, source=source, color='red', legend='test')\n",
    "factor_cmap(field_name='test', palette='Viridis256', factors=pivot.columns)\n",
    "\n",
    "hover = HoverTool(tooltips=[(\"Date\",\"@created_mdy\"),(\"Ceiling\",\"@Ceiling\")])\n",
    "linegraph.add_tools(hover)\n",
    "show(linegraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create graphs of each kind, format into dashboard\n",
    "complaint_count = BokehBarGraph(category='cleaned_descriptor', value='count').make_plot()\n",
    "month_hour = BokehHeatmap(x='created_week', y='created_hour', value='count').make_plot()\n",
    "geomap = BokehGeoMap().make_plot()\n",
    "rowdash = row(complaint_count, month_hour)\n",
    "#show(column(rowdash, month_hour, geomap))\n",
    "\n",
    "output_file('tab_test.html')\n",
    "tab1 = Panel(child=complaint_count, title='Complaint Count')\n",
    "tab2 = Panel(child=month_hour, title='Month Hour')\n",
    "tab3 = Panel(child=geomap, title='Geomap')\n",
    "\n",
    "tabs = Tabs(tabs=[tab1, tab2, tab3], background='white')\n",
    "\n",
    "show(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
