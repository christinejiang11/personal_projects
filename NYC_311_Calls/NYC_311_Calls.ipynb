{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ideas for importing data:\n",
    "#1. get data in batches: export to a list, export list to a csv, import csv for each new run - do data preprocessing\n",
    "#2. get data in batches, process each batch, export to a csv, import csv with specified datatypes\n",
    "#3. how big is a \"big\" dataset? whats the general limit for what i can export to work with on my computer?\n",
    "#4. multithreading vs. multiprocessing?\n",
    "#5. export data to a database? SQLalchemy\n",
    "#6. work directly in colab, then download and export to github?\n",
    "#\n",
    "#call with ori:\n",
    "#options:\n",
    "#1. download locally, wait for however long it takes, large = when it stops fitting in ram\n",
    "#mem = temp working space, fast to access but can't store much, might be erased; disk = long term storage\n",
    "#goal = get something done quickly\n",
    "#embarrassingly parallel problems: processes don't need to talk to each other \n",
    "\n",
    "#2. BQ: data stays in google servers, might have to pay eventually, if you need to do a lot of processing / really large datasets\n",
    "#take the processed results \n",
    "#use api to query from BQ\n",
    "\n",
    "#3. spark cluster on AWS\n",
    "\n",
    "#keep raw data - quickly recover\n",
    "\n",
    "#python stores each num as an obj; each obj has overhead involved (methods, value, ref count)\n",
    "#numpy stores all of these values as one obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###QUESTIONS\n",
    "#how to run bokeh server - will interactions work?\n",
    "#how to create multi-index heatmap\n",
    "#absolute paths?\n",
    "#running on heap cluster?\n",
    "\n",
    "#general questions:\n",
    "#how to deal with large datasets?\n",
    "#interactive sliders with bokeh server? i'm very confused about bokeh server\n",
    "#code review?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do:\n",
    "- multiprocessing or multithreading to get more data\n",
    "- save data to csv, use pandas to read in and indicate appropriate data types\n",
    "- data processing: add time, drop nulls\n",
    "- reduce memory\n",
    "- create cleaner categories; create function to indicate any new categories that need to be coded\n",
    "\n",
    "\n",
    "- new process flow:\n",
    "- import dataset, ingest columns with correct datatypes\n",
    "- preprocess and add new columns\n",
    "- specifically cast any columns you want to change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup / Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from sodapy import Socrata\n",
    "import os\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "from fuzzywuzzy import process\n",
    "from fuzzywuzzy import fuzz\n",
    "import pygsheets\n",
    "from datetime import datetime, date, time \n",
    "import json\n",
    "\n",
    "from bokeh.plotting import figure, output_file, output_notebook, show, save, reset_output, gmap\n",
    "from bokeh.models import ColumnDataSource, GMapOptions, HoverTool, BasicTicker, ColorBar, LinearColorMapper, PrintfTickFormatter, Panel, Tabs, CheckboxButtonGroup, CheckboxGroup, TextInput, Slider, DateRangeSlider\n",
    "from bokeh.palettes import Spectral6, all_palettes, brewer\n",
    "from bokeh.transform import factor_cmap, transform, linear_cmap\n",
    "from bokeh.layouts import column, row, layout, WidgetBox\n",
    "from bokeh.io import output_file, show\n",
    "from bokeh.sampledata.unemployment1948 import data\n",
    "from bokeh.application import Application\n",
    "from bokeh.application.handlers import FunctionHandler\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "pd.options.display.max_columns=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'count': '569447'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define parameters for endpoint, dataset, and app token\n",
    "data_url = 'data.cityofnewyork.us'\n",
    "dataset = 'erm2-nwe9'\n",
    "app_token = 'dM7DDeidbAmgtydtJVV1epbiU'\n",
    "\n",
    "#sets up the connection, need application token to override throttling limits\n",
    "#username and password only required for creating or modifying data\n",
    "client = Socrata(data_url, app_token)\n",
    "client.timeout = 6000\n",
    "\n",
    "#count number of records in desired dataset\n",
    "record_count = client.get(dataset, select='count(*)', where=\"created_date >='2020-02-01'\")\n",
    "record_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(chunk_size=100000, total_rows=500000):\n",
    "    start = 0\n",
    "    results=[]\n",
    "\n",
    "    #paginate through dataset in sets of 10000 to get all records since 2019\n",
    "    while True:\n",
    "        print(f'{start} rows retrieved')\n",
    "        results.extend(client.get(dataset,where=\"created_date >= '2020-02-01'\", \n",
    "                                  limit=chunk_size, offset=start))\n",
    "        start += chunk_size\n",
    "        if start > total_rows:\n",
    "            break\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Only run if getting new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orig_results = get_data()\n",
    "# orig_df = pd.DataFrame(orig_results)\n",
    "# path = '/Users/christinejiang/Documents/Python/data/'\n",
    "# orig_df.to_csv(path+'311_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reduce_memory(df):\n",
    "#     \"\"\"improved version of memory reduction function. uses pd.to_numeric to downcast types;\n",
    "#     also considers whether there are few enough unique values to warrant use of category vs. object.\"\"\"\n",
    "#     orig_size = df.memory_usage().sum()/1024**2\n",
    "#     dtypes = df.dtypes.astype(str).unique()\n",
    "#     converted_float = pd.DataFrame()\n",
    "#     converted_int = pd.DataFrame()\n",
    "#     converted_obj = pd.DataFrame()\n",
    "#     converted_misc = pd.DataFrame()\n",
    "\n",
    "#     #convert floats\n",
    "#     selected_float = df.select_dtypes(include='float')\n",
    "#     converted_float = selected_float.apply(pd.to_numeric, downcast='float')\n",
    "#     float_size = selected_float.memory_usage().sum()/1024**2\n",
    "#     converted_float_size = converted_float.memory_usage().sum()/1024**2\n",
    "#     print(f'floats: {float_size:.2f} reduced to {converted_float_size:.2f} MB')\n",
    "\n",
    "#     #convert ints\n",
    "#     selected_int = df.select_dtypes(include='integer')\n",
    "#     converted_int = selected_int.apply(pd.to_numeric, downcast='integer')\n",
    "#     int_size = selected_int.memory_usage().sum()/1024**2\n",
    "#     converted_int_size = converted_int.memory_usage().sum()/1024**2\n",
    "#     print(f'ints: {int_size:.2f} reduced to {converted_int_size:.2f} MB')\n",
    "    \n",
    "#     #convert objects / categories\n",
    "#     selected_object = df.select_dtypes(include=['object', 'category'])\n",
    "#     obj_size = selected_object.memory_usage().sum()/1024**2\n",
    "#     for col in selected_object.columns:\n",
    "#         count = len(selected_object[col])\n",
    "#         unique = len(selected_object[col].astype(str).unique())\n",
    "#         if unique < count/2:\n",
    "#             converted_obj[col] = selected_object[col].astype(str).astype('category')\n",
    "#         else:\n",
    "#             converted_obj[col] = selected_object[col].astype(str)\n",
    "#     converted_obj_size = converted_obj.memory_usage().sum()/1024**2\n",
    "#     print(f'object: {obj_size:.2f} reduced to {converted_obj_size:.2f} MB')\n",
    "\n",
    "#     #join floats, ints, and objects / categories\n",
    "#     float_int = converted_float.join(converted_int)\n",
    "#     float_int_obj = float_int.join(converted_obj)\n",
    "    \n",
    "#     #for any columns of any other type, keep them the same and join to the converted dataframe\n",
    "#     no_change_cols = [x for x in df.columns if x not in float_int_obj.columns]\n",
    "#     reduced_df = float_int_obj.join(df[no_change_cols])\n",
    "    \n",
    "#     #re-order columns to appear in original order\n",
    "#     reduced_df = reduced_df[df.columns]\n",
    "#     reduced_size = reduced_df.memory_usage().sum()/1024**2\n",
    "#     print(f'final df: {orig_size:.2f} reduced to {reduced_size:.2f} MB, {(orig_size-reduced_size)/orig_size*100:.1f}% reduction')\n",
    "#     return reduced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'linyu'\n",
    "path = '/Users/'+user+'/Documents/Python/data/'\n",
    "orig_df = pd.read_csv(path+'311_data.csv', usecols=['unique_key', 'created_date', 'closed_date', 'agency', 'agency_name',\n",
    "                                                 'complaint_type', 'descriptor', 'location_type', 'incident_zip',\n",
    "                                                 'address_type', 'city', 'status', 'latitude', 'longitude', 'location'], parse_dates=['created_date', 'closed_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #need to fix this - categorical blanks are not showing up as blanks\n",
    "# plt.rc('figure',figsize=(15,4))\n",
    "# #display(sns.heatmap(olddf.isnull()))\n",
    "# display(sns.heatmap(newdf.isnull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "- need to create a function that will show the new, unmapped categories for each new imported dataset\n",
    "- need to indicate column types upon reading in with pandas, use the reduce memory function after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ingest all datetime as datetimes\n",
    "#leave strings as objects, integers as ints, floats as floats\n",
    "#specifically cast columns you want to change to diff dtype\n",
    "#use categorical if you're going to be grouping by (cardinality<1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_vars(df):\n",
    "    #look into vectorized functions\n",
    "    #note: .apply not vectorized, slow\n",
    "    df['created_mdy'] = df['created_date'].dt.date\n",
    "    df['created_year'] = df['created_date'].dt.year\n",
    "    df['created_month'] = df['created_date'].dt.month\n",
    "    df['created_day'] = df['created_date'].dt.day\n",
    "    df['created_weekday'] = df['created_date'].dt.day_name()\n",
    "    df['created_week'] = df['created_date'].dt.week\n",
    "    df['created_hour'] = df['created_date'].dt.hour\n",
    "    df['days_to_close'] = (df['closed_date'] - df['created_date']).dt.days\n",
    "    df['count'] = 1\n",
    "    df['hour'] = [x.strftime('%I %p') for x in df['created_date']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = add_time_vars(orig_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_col_values(workbook, df, columns):\n",
    "    \"\"\"for a list of columns, creates a new sheet for each column and exports unique values and their counts to that sheet\"\"\"\n",
    "    for col in columns:\n",
    "        value_counts = df[col].value_counts()\n",
    "        counts_df = pd.DataFrame(value_counts).reset_index()\n",
    "        #was getting an error with using fillna for categorical column, need to cast to string\n",
    "        counts_df['index'] = counts_df['index'].astype(str)\n",
    "        try:\n",
    "            worksheet = workbook.worksheet_by_title(col)\n",
    "        except Exception:\n",
    "            #ensure the error is in regards to missing the worksheet\n",
    "            print(sys.exc_info())\n",
    "            workbook.add_worksheet(col)\n",
    "            worksheet = workbook.worksheet_by_title(col)\n",
    "        worksheet.set_dataframe(counts_df, start='A1')\n",
    "    print(f'{len(columns)} sets of column values exported.')\n",
    "        \n",
    "def get_valid_names(workbook, columns, start='D1'):\n",
    "    \"\"\"extracts the valid names manually entered by the user in column D of the workbook\"\"\"\n",
    "    valid_names = {}\n",
    "    for col in columns:\n",
    "        worksheet = workbook.worksheet_by_title(col)\n",
    "        valid_matrix = worksheet.get_values(start='D1', end='D100')\n",
    "        valid_names[col] = [v[0] for v in valid_matrix]\n",
    "    return valid_names\n",
    "\n",
    "def fuzzy_match(value):\n",
    "    \"\"\"returns the best match for each column; fuzzy match score of < 90 will return 'Other'\"\"\"\n",
    "    match = process.extract(query=value, choices=valid_names[col], limit=1)\n",
    "    if match[0][1] < 90:\n",
    "        return 'Other'\n",
    "    else:\n",
    "        return match[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 sets of column values exported.\n"
     ]
    }
   ],
   "source": [
    "#use pygsheets to connect to workbook where we will export unique column values\n",
    "client = pygsheets.authorize(service_account_file=path+'client_secret.json')\n",
    "workbook = client.open('311_data_cleaning')\n",
    "columns = ['agency_name','complaint_type','descriptor','location_type','city']\n",
    "\n",
    "#export unique column values and their counts\n",
    "export_col_values(workbook, new_df, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['agency_name'] = new_df['agency_name'].astype('category')\n",
    "new_df['complaint_type'] = new_df['complaint_type'].astype('category')\n",
    "new_df['descriptor'] = new_df['descriptor'].astype('category')\n",
    "new_df['location_type'] = new_df['location_type'].astype('category')\n",
    "new_df['city'] = new_df['city'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMMENT OUT THIS CELL IF DOING CATEGORY REVIEW\n",
    "#get dictionary of lists with valid names for each column\n",
    "valid_names = get_valid_names(workbook, columns, start='D1')\n",
    "\n",
    "#fuzzy match each of the columns to the available values\n",
    "for col in columns:\n",
    "    new_df['cleaned_'+col] = new_df[col].apply(fuzzy_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess dataset to remove mostly null columns and create date columns\n",
    "nyc_311_calls = new_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bokeh Visualizations\n",
    "- function to create bar graphs\n",
    "- function to create tables\n",
    "- function to create heatmaps\n",
    "- function to create geo map\n",
    "- function to create call outs\n",
    "- add visualizations and sliders to dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap with Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_tab(nyc_311_calls, x, y, value, title=None, x_ticks=None, y_ticks=None, exclude=None):\n",
    "    \n",
    "    def make_dataset(x, y, value):\n",
    "        pivot = nyc_311_calls.pivot_table(values=value, index=x, columns=y, aggfunc='sum')\n",
    "        pivot.columns = pivot.columns.astype(str)\n",
    "        pivot.index = pivot.index.astype(str)\n",
    "        if exclude:\n",
    "            try:\n",
    "                pivot = pivot.drop(exclude)\n",
    "                print(f'{exclude} dropped from index.')\n",
    "            except KeyError:\n",
    "                print(f'{exclude} dropped from columns.')\n",
    "                pivot = pivot.drop(exclude, axis=1)\n",
    "            except:\n",
    "                print('Exclusion does not exist in index or columns.')\n",
    "        df_pivot = pd.DataFrame(pivot.stack()).reset_index()\n",
    "        df_pivot.columns = ['y','x','value']\n",
    "        return ColumnDataSource(df_pivot)\n",
    "    \n",
    "    def make_plot(src):\n",
    "        #output_notebook()\n",
    "        colors = [\"#75968f\", \"#a5bab7\", \"#c9d9d3\", \"#e2e2e2\", \"#dfccce\", \"#ddb7b1\", \"#cc7878\", \"#933b41\", \"#550b1d\"]\n",
    "        mapper = LinearColorMapper(palette=colors, low=src.data['value'].min(), high=src.data['value'].max())\n",
    "        \n",
    "        if x_ticks:\n",
    "            x_range = x_ticks\n",
    "        else:\n",
    "            x_range = sorted(list(set(src.data['x'])))\n",
    "        \n",
    "        if y_ticks:\n",
    "            y_range = y_ticks\n",
    "        else:\n",
    "            y_range = sorted(list(set(src.data['y'])))\n",
    "        \n",
    "        p = figure(plot_width=900, plot_height=500, x_range=x_range, \n",
    "                   y_range = y_range, title = title)\n",
    "        p.rect(x='x', y='y', width=1, height=1, source=src, line_color='white', fill_color=transform('value', mapper))\n",
    "        color_bar = ColorBar(color_mapper=mapper, location=(0,0), ticker=BasicTicker(desired_num_ticks=len(colors)))\n",
    "        p.add_layout(color_bar, 'right')\n",
    "        hover = HoverTool(tooltips=[(x,'@x'),(y,'@y'), (value,'@value')])\n",
    "        p.add_tools(hover)\n",
    "        p.grid.visible=False\n",
    "        p.axis.axis_line_color = None\n",
    "        p.axis.major_tick_line_color = None\n",
    "        p.axis.major_label_text_font_size = \"11px\"\n",
    "        p.axis.major_label_standoff = 0\n",
    "        p.xaxis.major_label_orientation = 1.0\n",
    "        return p\n",
    "    \n",
    "    def update(attr, old, new):\n",
    "        dates_to_plot = date_range_slider.value\n",
    "        \n",
    "    date_range_slider = DateRangeSlider(title=\"Date Range: \", start=date(2020, 1, 1), end=date.today(), value=(date(2020, 1, 1), date.today()), step=1)\n",
    "    date_range_slider.on_change('value',update)\n",
    "                                        \n",
    "    src = make_dataset(x, y, value)\n",
    "    plot = make_plot(src)\n",
    "    \n",
    "    controls=WidgetBox(date_range_slider)\n",
    "    layout = column(controls, plot)\n",
    "    tab = Panel(child=layout, title='Heatmap')\n",
    "    return tab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2020, 1, 1)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_range_slider = DateRangeSlider(title=\"Date Range: \", start=date(2020, 1, 1), end=date.today(), value=(date(2020, 1, 1), date.today()), step=1)\n",
    "date_range_slider.value[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geomap with Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make_dataset() format specific data to be displayed\n",
    "#make_plot() draw plot with specified data\n",
    "#update() update plot based on user selections\n",
    "\n",
    "def geomap_tab(nyc_311_calls):\n",
    "    #function to make dataset for geomap based on number of points to display\n",
    "    #make_dataset function has the params that can be used to change the underlying data\n",
    "    #params: neighborhood, number of points\n",
    "    def make_dataset(boroughs, display_num=10000):\n",
    "        borough_filter = nyc_311_calls[nyc_311_calls['borough'].isin(boroughs)]\n",
    "        borough_filter['lat_round'] = round(borough_filter['latitude'],3)\n",
    "        borough_filter['lon_round'] = round(borough_filter['longitude'],3)\n",
    "        latlon_df = pd.DataFrame(borough_filter.groupby(['lat_round', 'lon_round'])['count'].sum()).reset_index()\n",
    "        latlon_df['sizes'] = latlon_df['count']/latlon_df['count'].max()*50\n",
    "        latlon_sorted = latlon_df.sort_values('sizes', ascending=False)\n",
    "        latlon_display = latlon_sorted[:display_num]\n",
    "        return ColumnDataSource(latlon_display)\n",
    "    \n",
    "    def make_plot(src):\n",
    "        #output_notebook()\n",
    "        #function to generate the plot given a ColumnDataSource with specific parameters\n",
    "        with open(path+'client_secret.json') as f:\n",
    "            data = json.load(f)\n",
    "        api_key = data['google_api_key']\n",
    "        map_options = GMapOptions(lat=40.76, lng=-73.95, map_type='roadmap', zoom=11)\n",
    "        call_map = gmap(api_key, map_options, title='NYC 311 Calls')\n",
    "        call_map.circle(x='lon_round', y='lat_round', size='sizes', source=src, fill_alpha = 0.7, \n",
    "                        fill_color='salmon', line_color='red')\n",
    "        \n",
    "        hover = HoverTool(tooltips=[('longitude','@lon_round'),('latitude','@lat_round'), ('count','@count')])\n",
    "        call_map.add_tools(hover)\n",
    "        return call_map\n",
    "    \n",
    "    def update(attr, old, new):\n",
    "        #function to update the underlying dataset given any change in interface widgets (checkbox, slider)\n",
    "        boroughs_to_plot = [borough_selection.labels[i] for i in borough_selection.active]\n",
    "        num_display = display_selection.value\n",
    "        new_src = make_dataset(boroughs_to_plot, display_num=num_display)\n",
    "        src.data.update(new_src.data)\n",
    "        \n",
    "    available_boroughs = list(set(nyc_311_calls['borough']))\n",
    "    available_boroughs.sort()\n",
    "    \n",
    "    borough_selection = CheckboxGroup(labels=available_boroughs, active=[0,1,2,3,4])\n",
    "    borough_selection.on_change('active', update)\n",
    "    \n",
    "    display_selection = Slider(start=1000, end=5000, step=100, value=5000, title='Display Num')\n",
    "    display_selection.on_change('value', update)\n",
    "    \n",
    "    date_range_slider = DateRangeSlider(title=\"Date Range: \", start=date(2020, 1, 1), end=date.today(), value=(date(2020, 1, 1), date.today()), step=1)\n",
    "    \n",
    "    initial_boroughs = [borough_selection.labels[i] for i in borough_selection.active]\n",
    "    \n",
    "    src = make_dataset(initial_boroughs, display_num=display_selection.value)\n",
    "    p = make_plot(src)\n",
    "    controls = WidgetBox(date_range_slider, borough_selection, display_selection)\n",
    "    layout = row(controls, p)\n",
    "    tab = Panel(child=layout, title='Geomap')\n",
    "    tabs = Tabs(tabs=[tab])\n",
    "    return tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Graph with Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bargraph_tab(nyc_311_calls, categories=['cleaned_descriptor']):\n",
    "    \n",
    "    def make_dataset(category, value, num_categories):\n",
    "        grouped_df = pd.DataFrame(nyc_311_calls.groupby(category)[value].sum()).reset_index()\n",
    "        grouped_df[category] = grouped_df[category].astype(str)\n",
    "        grouped_df = grouped_df.sort_values(value, ascending = False)[:num_categories]\n",
    "        colors = ['#550b1d', '#933b41', '#cc7878', '#ddb7b1', '#dfccce', '#e2e2e2', '#c9d9d3', '#a5bab7', '#75968f']\n",
    "        try:\n",
    "            grouped_df['color'] = colors[:num_categories]\n",
    "        except KeyError:\n",
    "            print('Too many categories selected; select up to 9 categories to display')\n",
    "        return ColumnDataSource(grouped_df)\n",
    "        \n",
    "    def make_plot(src):\n",
    "        output_file = ('test.html')\n",
    "        p = figure(y_range = list(reversed(src.data[category])), plot_height=300, plot_width=600, \n",
    "                  x_axis_label='Calls')\n",
    "        \n",
    "#         mapper = LinearColorMapper(palette=colors, low=src.data['value'].min(), high=src.data['value'].max())\n",
    "        p.hbar(y=category, right=value, source=src, height=0.8, color='color', hover_color='white')\n",
    "        category_value = f'@{category}'\n",
    "        value_value = f'@{value}'\n",
    "        hover = HoverTool(tooltips=[(category, category_value), (value, value_value)])\n",
    "        p.add_tools(hover)\n",
    "        return p\n",
    "        \n",
    "    def update():\n",
    "        pass\n",
    "    \n",
    "    value='count'\n",
    "    num_categories=8\n",
    "    \n",
    "    date_range_slider = DateRangeSlider(title=\"Date Range: \", start=date(2020, 1, 1), end=date.today(), value=(date(2020, 1, 1), date.today()), step=1)\n",
    "    controls = WidgetBox(date_range_slider)\n",
    "\n",
    "    plots = {}\n",
    "    for i, category in enumerate(categories):\n",
    "        src = make_dataset(category, value, num_categories)\n",
    "        p = make_plot(src)\n",
    "        plots[i] = p\n",
    "    \n",
    "    row_1 = row(plots[2], plots[3])\n",
    "    row_2 = row(plots[0], plots[1])\n",
    "    layout = column(controls, row_1, row_2)\n",
    "    tab = Panel(child=layout, title='Bargraph')\n",
    "    tabs = Tabs(tabs=[tab])\n",
    "    return tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other dropped from index.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'borough'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'borough'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-745a69d6b8d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mheatmap2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheatmap_tab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnyc_311_calls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cleaned_descriptor'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'created_week'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Calls by Type'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Other'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mgeomap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeomap_tab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnyc_311_calls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mbargraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbargraph_tab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnyc_311_calls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cleaned_agency_name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'cleaned_descriptor'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'cleaned_city'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cleaned_location_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-c23bca298fc7>\u001b[0m in \u001b[0;36mgeomap_tab\u001b[0;34m(nyc_311_calls)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mavailable_boroughs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnyc_311_calls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'borough'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mavailable_boroughs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2800\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2646\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'borough'"
     ]
    }
   ],
   "source": [
    "time = ['11 PM', '10 PM', '09 PM', '08 PM', '07 PM', '06 PM', '05 PM', '04 PM', '03 PM', '02 PM', '01 PM', '12 PM', '11 AM', '10 AM', '09 AM', '08 AM', '07 AM', '06 AM', '05 AM', '04 AM', '03 AM', '02 AM', '01 AM', '12 AM']\n",
    "\n",
    "heatmap = heatmap_tab(nyc_311_calls, 'hour','created_weekday','count', 'Calls by Day and Hour', y_ticks=time,\n",
    "                     x_ticks=['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat'])\n",
    "\n",
    "heatmap2 = heatmap_tab(nyc_311_calls, 'cleaned_descriptor','created_week','count', 'Calls by Type', exclude='Other')\n",
    "\n",
    "geomap = geomap_tab(nyc_311_calls)\n",
    "\n",
    "bargraph = bargraph_tab(nyc_311_calls, ['cleaned_agency_name','cleaned_descriptor','cleaned_city', 'cleaned_location_type'])\n",
    "\n",
    "tabs = Tabs(tabs=[bargraph, heatmap, heatmap2, geomap])\n",
    "output_file('dashboard.html')\n",
    "#show(tabs)\n",
    "curdoc().add_root(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BokehHeatmap:\n",
    "#     def __init__(self, x, y, value, exclude=None):\n",
    "#         self.x = x\n",
    "#         self.y = y\n",
    "#         self.value = value\n",
    "#         self.exclude = exclude\n",
    "#         self.pivot = self.make_dataset()[0]\n",
    "#         self.df_pivot = self.make_dataset()[1]\n",
    "        \n",
    "#     def make_dataset(self):\n",
    "#         value = self.value\n",
    "#         x=self.x\n",
    "#         y=self.y\n",
    "#         exclude = self.exclude\n",
    "#         pivot = clean_processed.pivot_table(values=value,index=x,columns=y,aggfunc='sum')\n",
    "#         if exclude:\n",
    "#             try:\n",
    "#                 pivot = pivot.drop(exclude, axis=0)\n",
    "#                 print(f'{exclude} dropped from x-axis.')\n",
    "#             except KeyError:\n",
    "#                 pivot = pivot.drop(exclude, axis=1)\n",
    "#                 print(f'{exclude} dropped from y-axis.')\n",
    "#             except:\n",
    "#                 print('Exclusion does not exist in index or columns.')\n",
    "#         pivot.columns = pivot.columns.astype(str)\n",
    "#         pivot.index = pivot.index.astype(str)\n",
    "#         list(pivot.index)\n",
    "#         df_pivot = pd.DataFrame(pivot.stack(), columns=[value]).reset_index()\n",
    "#         return pivot, df_pivot\n",
    "\n",
    "#     def make_plot(self, title=None, x_ticks=None, y_ticks=None):\n",
    "#         #reset_output()\n",
    "#         #output_notebook()\n",
    "#         df_pivot = self.df_pivot\n",
    "#         pivot = self.pivot\n",
    "#         x = self.x\n",
    "#         y = self.y\n",
    "#         value = self.value \n",
    "#         source = ColumnDataSource(df_pivot)\n",
    "        \n",
    "#         if title:\n",
    "#             graph_title = title\n",
    "#         else:\n",
    "#             graph_title= f'{value} by {x} and {y}'\n",
    "        \n",
    "#         if x_ticks:\n",
    "#             x_range = x_ticks\n",
    "#         else:\n",
    "#             x_range = list(pivot.index)\n",
    "            \n",
    "#         if y_ticks:\n",
    "#             y_range = y_ticks\n",
    "#         else:\n",
    "#             y_range = list((pivot.columns))\n",
    "\n",
    "#         colors = [\"#75968f\", \"#a5bab7\", \"#c9d9d3\", \"#e2e2e2\", \"#dfccce\", \"#ddb7b1\", \"#cc7878\", \"#933b41\", \"#550b1d\"]\n",
    "#         mapper = LinearColorMapper(palette=colors, low=df_pivot[value].min(), high=df_pivot[value].max())\n",
    "\n",
    "#         p = figure(plot_width=900, plot_height=500, title=graph_title,\n",
    "#                    x_range=x_range, y_range=y_range,\n",
    "#                    toolbar_location=None, x_axis_location=\"below\")\n",
    "\n",
    "#         p.rect(x=x, y=y, width=1, height=1, source=source, line_color='white', fill_color=transform(value, mapper))\n",
    "#         color_bar = ColorBar(color_mapper=mapper, location=(0, 0),\n",
    "#                              ticker=BasicTicker(desired_num_ticks=len(colors)))\n",
    "#         p.add_layout(color_bar, 'right')\n",
    "        \n",
    "#         value_value = f'@{value}'\n",
    "#         x_value = f'@{x}'\n",
    "#         y_value = f'@{y}'\n",
    "\n",
    "#         hover = HoverTool(tooltips=[(x, x_value),(y, y_value), (value, value_value)])\n",
    "#         p.add_tools(hover)\n",
    "#         p.grid.visible=False\n",
    "\n",
    "#         p.axis.axis_line_color = None\n",
    "#         p.axis.major_tick_line_color = None\n",
    "#         p.axis.major_label_text_font_size = \"11px\"\n",
    "#         p.axis.major_label_standoff = 0\n",
    "#         p.xaxis.major_label_orientation = 1.0\n",
    "#         return p\n",
    "\n",
    "# class BokehGeoMap():\n",
    "#     def __init__(self, color='salmon', display_number=5000):\n",
    "#         self.color = color\n",
    "#         self.display_number = display_number\n",
    "#         self.dataset = self.make_dataset()\n",
    "        \n",
    "#     def make_dataset(self):\n",
    "#         clean_processed['round_lat'] = round(clean_processed['latitude'],3)\n",
    "#         clean_processed['round_lon'] = round(clean_processed['longitude'],3)\n",
    "#         latlon_df = pd.DataFrame(clean_processed.groupby(['round_lat', 'round_lon'])['count'].sum()).reset_index()\n",
    "#         latlon_df.sort_values('count', ascending=False)\n",
    "#         latlon_df['scaled'] = (latlon_df['count']/latlon_df['count'].max()*50)\n",
    "#         latlon_sorted = latlon_df.sort_values('scaled',ascending=False)\n",
    "#         return latlon_sorted\n",
    "    \n",
    "#     def make_plot(self):\n",
    "#         #output_file('maps.html')\n",
    "#         color = self.color\n",
    "#         display_number = self.display_number\n",
    "#         dataset = self.dataset\n",
    "#         sample_df = dataset[:display_number]\n",
    "        \n",
    "#         latitudes = sample_df['round_lat']\n",
    "#         longitudes = sample_df['round_lon']\n",
    "#         sizes = sample_df['scaled']\n",
    "\n",
    "#         with open(path+'client_secret.json') as f:\n",
    "#             data = json.load(f)\n",
    "#         api_key = data[\"google_api_key\"]\n",
    "\n",
    "#         map_options = GMapOptions(lat=40.76, lng=-73.95, map_type=\"roadmap\", zoom=11)\n",
    "#         call_map = gmap(api_key, map_options, title=\"NYC 311 Calls\")\n",
    "#         source = ColumnDataSource(data=dict(lat=latitudes,lon=longitudes, sizes = sizes))\n",
    "#         call_map.circle(x=\"lon\", y=\"lat\", size='sizes', fill_color=color, fill_alpha=0.7, line_color=color, source=source)\n",
    "#         return call_map\n",
    "\n",
    "\n",
    "#class BokehBarGraph:\n",
    "#     def __init__(self, category, value, num_categories=9):\n",
    "#         self.category = category\n",
    "#         self.value = value\n",
    "#         self.num_categories = num_categories\n",
    "#         self.dataset = self.make_dataset()\n",
    "        \n",
    "#     def make_dataset(self):\n",
    "#         category = self.category\n",
    "#         value = self.value\n",
    "#         num_categories = self.num_categories\n",
    "#         grouped_df = pd.DataFrame(clean_processed.groupby(category)[value].sum()).reset_index()[0:num_categories]\n",
    "#         grouped_df[category] = grouped_df[category].astype(str)\n",
    "#         grouped_df = grouped_df.sort_values(value, ascending=True)\n",
    "#         try:\n",
    "#             grouped_df['color'] = brewer['YlGnBu'][num_categories]\n",
    "#         except KeyError:\n",
    "#             print('Too many categories selected; select up to 9 categories to display.')\n",
    "#         return grouped_df\n",
    "    \n",
    "#     def make_plot(self):\n",
    "#         category = self.category\n",
    "#         value = self.value\n",
    "#         dataset = self.dataset\n",
    "#         #output_file('bargraph.html')\n",
    "#         source = ColumnDataSource(dataset)\n",
    "\n",
    "#         bargraph = figure(y_range=dataset[category], plot_height = 300, plot_width=600, background_fill_color=\"#000000\",\n",
    "#                    x_axis_label=value, y_axis_label=category)\n",
    "#         bargraph.grid.visible=False\n",
    "        \n",
    "# #         color_map = factor_cmap(field_name=category, palette = Spectral6, \n",
    "# #                                 factors=value_count[category].unique())\n",
    "#         bargraph.hbar(y=category,right=value,source=source, height=.8, color='color', hover_color='white')\n",
    "#         category_value = f'@{category}'\n",
    "#         value_value = f'@{value}'\n",
    "#         hover = HoverTool(tooltips=[(category,category_value),(value, value_value)])\n",
    "#         bargraph.add_tools(hover)\n",
    "#         return bargraph\n",
    "#         #show(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##line graph\n",
    "# clean_processed['cleaned_descriptor'] = clean_processed['cleaned_descriptor'].astype(str)\n",
    "# clean_processed['created_mdy'] = clean_processed['created_mdy'].astype(str)\n",
    "# pivot = clean_processed.pivot_table(values='count', index='created_mdy', columns='cleaned_descriptor', aggfunc=sum).reset_index()\n",
    "# source = ColumnDataSource(pivot)\n",
    "\n",
    "# reset_output()\n",
    "# linegraph = figure(x_range=pivot['created_mdy'])\n",
    "# for col in pivot.columns:\n",
    "#     linegraph.line(x='created_mdy', y = col, source=source, color='red', legend='test')\n",
    "# factor_cmap(field_name='test', palette='Viridis256', factors=pivot.columns)\n",
    "\n",
    "# hover = HoverTool(tooltips=[(\"Date\",\"@created_mdy\"),(\"Ceiling\",\"@Ceiling\")])\n",
    "# linegraph.add_tools(hover)\n",
    "# show(linegraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
