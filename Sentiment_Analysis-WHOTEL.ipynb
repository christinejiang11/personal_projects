{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis for Whispr\n",
    "\n",
    "1. import data from google sheets\n",
    "2. clean dataset and create synthetic variables\n",
    "3. summarize dataset: how many records per category, reviews over time\n",
    "4. evaluate sentiment of review, give confidence interval\n",
    "5. calculate summary insights: average sentiment / subjectivity per item, reviews per item\n",
    "6. compare against manual evaluation\n",
    "7. export data to google sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "import gspread\n",
    "import pygsheets\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag_sents, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet, stopwords, words\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid')\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Import data from GS using GSpread\n",
    "- connect to google sheets API\n",
    "- create spreadsheet and worksheet objects, explore GSpread library\n",
    "- create dataframe of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 define the scope of your access tokens\n",
    "scope = ['https://www.googleapis.com/auth/drive','https://spreadsheets.google.com/feeds']\n",
    "\n",
    "#2 after getting oauth2 credentials in a json, obtain an access token from google authorization server\n",
    "#by creating serviceaccountcredentials and indicating scope, which controls resources / operations that an\n",
    "#access token permits\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name('client_secret.json', scope)\n",
    "\n",
    "#3 log into the google API using oauth2 credentials\n",
    "#returns gspread.Client instance\n",
    "c = gspread.authorize(creds)\n",
    "\n",
    "spreadsheet = c.open('UK Sentiment')\n",
    "worksheet = spreadsheet.worksheet('WHotel_Sentiment')\n",
    "records = worksheet.get_all_records()\n",
    "df = pd.DataFrame(records)\n",
    "df = df[['Contents','Sentiment','Topic','Location','Comment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Import data from GS using pygsheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#authorization in one step - read client_secret\n",
    "gc = pygsheets.authorize(service_file='client_secret.json')\n",
    "spreadsheet2 = gc.open('UK Sentiment')\n",
    "\n",
    "#clean up workbook \n",
    "for item in spreadsheet2.worksheets():\n",
    "    title = item.title\n",
    "    if item.title not in ['UK_Reviews','WHotel_Sentiment','WHOTELS_analyzed']:\n",
    "        worksheet2 = spreadsheet2.worksheet_by_title(str(item.title))\n",
    "        spreadsheet2.del_worksheet(worksheet2)\n",
    "        print('{} sheet deleted'.format(item.title))\n",
    "        \n",
    "worksheet2 = spreadsheet2.worksheet_by_title('WHotel_Sentiment')\n",
    "records2 = worksheet2.get_all_records()\n",
    "df2 = pd.DataFrame(records2)\n",
    "df2 = df2[['Contents','Sentiment','Topic','Location','Comment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Simple sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christinejiang/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version. Use                 named aggregation instead.\n",
      "\n",
      "    >>> grouper.agg(name_1=func_1, name_2=func_2)\n",
      "\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentiment_Category</th>\n",
       "      <th>Textblob_Score</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td rowspan=\"3\" valign=\"top\">Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>-0.229419</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.003046</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Positive</td>\n",
       "      <td>0.379133</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.028125</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td rowspan=\"3\" valign=\"top\">Positive</td>\n",
       "      <td>Negative</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Positive</td>\n",
       "      <td>0.425419</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       mean  count\n",
       "Sentiment_Category Textblob_Score                 \n",
       "Negative           Negative       -0.229419   11.0\n",
       "                   Neutral         0.003046   72.0\n",
       "                   Positive        0.379133   55.0\n",
       "Neutral            Neutral         0.028125    1.0\n",
       "Positive           Negative       -0.400000    1.0\n",
       "                   Neutral         0.001145   14.0\n",
       "                   Positive        0.425419   20.0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#baseline sentiment analysis - use textblob polarity, compare accuracy\n",
    "df['Sentiment_Category'] = df['Sentiment'].map({1: 'Positive',2:'Neutral',3:'Negative'})\n",
    "\n",
    "def pos_neg(polarity):\n",
    "    if polarity >= 0.1:\n",
    "        return 'Positive'\n",
    "    if polarity >= 0 and polarity < 0.1:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Negative'\n",
    "\n",
    "df['Polarity'] = [TextBlob(x).polarity for x in df['Contents']]\n",
    "df['Subjectivity'] = [TextBlob(x).subjectivity for x in df['Contents']]\n",
    "df['Textblob_Score'] = df['Polarity'].apply(pos_neg)\n",
    "\n",
    "df.groupby(['Sentiment_Category','Textblob_Score'])['Polarity'].agg({'mean':np.mean, 'count':len})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. KNN Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create three checks: stopwords, punctuation, english\n",
    "mystop = stopwords.words('english')\n",
    "punctuation = string.punctuation\n",
    "englishwords = [x.lower() for x in words.words()]\n",
    "\n",
    "#lemmatize words in comments\n",
    "wnl = WordNetLemmatizer()\n",
    "myblob = TextBlob(str(df['Contents'].values.tolist())).tokenize()\n",
    "lemmatized = [wnl.lemmatize(x).lower() for x in myblob]\n",
    "\n",
    "#create list of lemmatized words\n",
    "finalwords = [word for word in lemmatized if word not in punctuation and word not in mystop and word in englishwords]\n",
    "\n",
    "#for lemmatized words, create counts and polarity scores\n",
    "counts = {x: finalwords.count(x) for x in finalwords}\n",
    "word_df = pd.DataFrame(counts.items(), columns = ['word','count']).sort_values('count', ascending = False)\n",
    "word_df['polarity'] = word_df['word'].apply(lambda x: TextBlob(x).polarity)\n",
    "positives = word_df[word_df['polarity']>0].sort_values(['count','polarity'], ascending = False)\n",
    "negatives = word_df[word_df['polarity']<0].sort_values(['count','polarity'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "toptenpos=positives.nlargest(20, columns='count')\n",
    "toptenneg=negatives.nlargest(20, columns='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and then the night come # bali balitrio # seminyak # baliseminyak # islandofgod # seminyakbali # wbali # woobar # tanskin # beachtravellers # beachlover # beach # beachbabes # beach # beachvacay # beachlovers # beachwaves # bikini # bikinigirls # wanderlust # wandersoul # woundedsoul # asianhotties # eurasianhotties # eurasianbabes # eurasianhotties # indonesian # indonesia # indonesiangirl # asiangirls'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lemmatize sentence\n",
    "#tokenize a sentence, tag it with its pos tags\n",
    "#for specific letters, convert to wn pos\n",
    "#lemmatize each word according to its pos\n",
    "#return a sentence of the lemmatized words\n",
    "#use this to convert each of the records in 'contents'\n",
    "#count the frequency of lemmatized words in contents\n",
    "#evaluate polarity\n",
    "#count frequency of top 10 lemmatized words in contents\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def nltk2wn(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None        \n",
    "    \n",
    "def lemmatize_sent(sentence):\n",
    "    nltk_tagged = pos_tag([x.lower() for x in nltk.word_tokenize(sentence)])\n",
    "    converted_tags = [(x[0], nltk2wn(x[1])) for x in nltk_tagged]\n",
    "    lemmatized_sent = []\n",
    "    for x in converted_tags:\n",
    "        if x[1] is None:\n",
    "            lemmatized_sent.append(x[0])\n",
    "        else:\n",
    "            lemmatized_sent.append(lemmatizer.lemmatize(x[0], pos = x[1]))                     \n",
    "    final_sentence = ' '.join(lemmatized_sent)\n",
    "    return final_sentence \n",
    "\n",
    "sentence = \"And then the night comes #bali balitrio #seminyak #baliseminyak #islandofgod #seminyakbali #wbali #woobar #tanskin #beachtravellers #beachlover #beach #beachbabes #beaches #beachvacay #beachlovers #beachwaves #bikini #bikinigirls #wanderlust #wandersoul #woundedsoul #asianhotties #eurasianhotties #eurasianbabes #eurasianhotties #indonesian #indonesia #indonesiangirl #asiangirls\"\n",
    "lemmatize_sent(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet.del_worksheet(analysis)\n",
    "spreadsheet.add_worksheet('WHOTELS_analyzed', rows = 200, cols = 10)\n",
    "analysis = spreadsheet.worksheet('WHOTELS_analyzed')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('His', 'PRP$'), ('latest', 'JJS'), ('song', 'NN'), ('was', 'VBD'), ('a', 'DT'), ('personal', 'JJ'), ('best', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag_sents\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text = \"The goal was to best the competition. His latest song was a personal best. Hence, he received the best song of the year award. He played best after a couple of martinis.\"\n",
    "text_sentence_tokens = sent_tokenize(text)\n",
    "#print(text_sentence_tokens)\n",
    "\n",
    "text_word_tokens = []\n",
    "for sentence_token in text_sentence_tokens:\n",
    "    text_word_tokens.append(word_tokenize(sentence_token))\n",
    "#print(text_word_tokens)\n",
    "\n",
    "text_tagged = pos_tag_sents(text_word_tokens)\n",
    "print(text_tagged[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
